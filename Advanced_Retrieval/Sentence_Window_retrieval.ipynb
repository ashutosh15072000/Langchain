{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Using cached llama_index-0.10.65-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index)\n",
      "  Using cached llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
      "  Using cached llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.65 (from llama_index)\n",
      "  Using cached llama_index_core-0.10.65-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
      "  Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama_index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
      "  Using cached llama_index_legacy-0.9.48.post1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.27 (from llama_index)\n",
      "  Using cached llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
      "  Using cached llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
      "  Using cached llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama_index)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
      "  Using cached openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in a:\\gpt\\gpt\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (3.9.5)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.6.0)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (3.1)\n",
      "Collecting nltk>=3.8.2 (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached nltk-3.8.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (1.24.3)\n",
      "Requirement already satisfied: pandas in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (2.31.0)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama_index) (4.11.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama_index)\n",
      "  Using cached llama_cloud-0.0.13-py3-none-any.whl.metadata (751 bytes)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.2.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama_index)\n",
      "  Using cached llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama_index) (4.0.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pydantic>=1.10 in a:\\gpt\\gpt\\lib\\site-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (2.7.4)\n",
      "Collecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.7)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: click in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.65->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.4.28)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
      "  Downloading jiter-0.5.0-cp310-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\gpt\\gpt\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in a:\\gpt\\gpt\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in a:\\gpt\\gpt\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama_index) (3.0.3)\n",
      "Requirement already satisfied: colorama in a:\\gpt\\gpt\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.65->llama_index) (0.4.6)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama_index)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in a:\\gpt\\gpt\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.2.1)\n",
      "Requirement already satisfied: packaging>=17.0 in a:\\gpt\\gpt\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama_index) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in a:\\gpt\\gpt\\lib\\site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in a:\\gpt\\gpt\\lib\\site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama_index) (2.18.4)\n",
      "Requirement already satisfied: six>=1.5 in a:\\gpt\\gpt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.65->llama_index) (1.16.0)\n",
      "Using cached llama_index-0.10.65-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_core-0.10.65-py3-none-any.whl (15.5 MB)\n",
      "Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post1-py3-none-any.whl (1.2 MB)\n",
      "Using cached llama_index_llms_openai-0.1.29-py3-none-any.whl (11 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.0.13-py3-none-any.whl (169 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
      "Using cached nltk-3.8.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.40.6-py3-none-any.whl (361 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.5.0-cp310-none-win_amd64.whl (190 kB)\n",
      "   ---------------------------------------- 0.0/191.0 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/191.0 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 61.4/191.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 163.8/191.0 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 191.0/191.0 kB 1.4 MB/s eta 0:00:00\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, wrapt, tenacity, soupsieve, sniffio, mypy-extensions, marshmallow, jiter, h11, distro, typing-inspect, nltk, httpcore, deprecated, beautifulsoup4, anyio, httpx, dataclasses-json, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.4.0\n",
      "    Uninstalling tenacity-8.4.0:\n",
      "      Successfully uninstalled tenacity-8.4.0\n",
      "Successfully installed anyio-4.4.0 beautifulsoup4-4.12.3 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 llama-cloud-0.0.13 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.65 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48.post1 llama-index-llms-openai-0.1.29 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 llama_index-0.10.65 marshmallow-3.21.3 mypy-extensions-1.0.0 nltk-3.8.2 openai-1.40.6 sniffio-1.3.1 soupsieve-2.5 striprtf-0.0.26 tenacity-8.5.0 typing-inspect-0.9.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-huggingface\n",
      "  Using cached llama_index_embeddings_huggingface-0.2.3-py3-none-any.whl.metadata (769 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.23.4)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.10.65)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-embeddings-huggingface) (3.0.1)\n",
      "Requirement already satisfied: filelock in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.11.0)\n",
      "Requirement already satisfied: aiohttp in a:\\gpt\\gpt\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.5)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Using cached minijinja-2.0.1-cp38-abi3-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in a:\\gpt\\gpt\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.30)\n",
      "Requirement already satisfied: dataclasses-json in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.1)\n",
      "Requirement already satisfied: nltk>=3.8.2 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.24.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.40.6)\n",
      "Requirement already satisfied: pandas in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (10.3.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in a:\\gpt\\gpt\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.41.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in a:\\gpt\\gpt\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in a:\\gpt\\gpt\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.0)\n",
      "Requirement already satisfied: scipy in a:\\gpt\\gpt\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
      "Requirement already satisfied: click in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.4.28)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.7.4)\n",
      "Requirement already satisfied: sniffio in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: certifi in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
      "Requirement already satisfied: idna in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in a:\\gpt\\gpt\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\gpt\\gpt\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in a:\\gpt\\gpt\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in a:\\gpt\\gpt\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
      "Requirement already satisfied: sympy in a:\\gpt\\gpt\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.12)\n",
      "Requirement already satisfied: jinja2 in a:\\gpt\\gpt\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in a:\\gpt\\gpt\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2021.4.0)\n",
      "Requirement already satisfied: colorama in a:\\gpt\\gpt\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in a:\\gpt\\gpt\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in a:\\gpt\\gpt\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in a:\\gpt\\gpt\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in a:\\gpt\\gpt\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in a:\\gpt\\gpt\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in a:\\gpt\\gpt\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in a:\\gpt\\gpt\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in a:\\gpt\\gpt\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2021.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in a:\\gpt\\gpt\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in a:\\gpt\\gpt\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.18.4)\n",
      "Requirement already satisfied: six>=1.5 in a:\\gpt\\gpt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in a:\\gpt\\gpt\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in a:\\gpt\\gpt\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Using cached llama_index_embeddings_huggingface-0.2.3-py3-none-any.whl (8.6 kB)\n",
      "Using cached minijinja-2.0.1-cp38-abi3-win_amd64.whl (752 kB)\n",
      "Installing collected packages: minijinja, llama-index-embeddings-huggingface\n",
      "Successfully installed llama-index-embeddings-huggingface-0.2.3 minijinja-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-vector-stores-qdrant\n",
      "  Downloading llama_index_vector_stores_qdrant-0.2.15-py3-none-any.whl.metadata (768 bytes)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-vector-stores-qdrant) (1.63.0)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-vector-stores-qdrant) (0.10.41)\n",
      "Collecting qdrant-client>=1.7.1 (from llama-index-vector-stores-qdrant)\n",
      "  Downloading qdrant_client-1.11.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in a:\\langchain\\langchainenv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2024.3.1)\n",
      "Requirement already satisfied: httpx in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.8.1)\n",
      "Requirement already satisfied: numpy in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.30.5)\n",
      "Requirement already satisfied: pandas in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.9.0)\n",
      "Requirement already satisfied: wrapt in a:\\langchain\\langchainenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.16.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (1.63.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.8.2)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in a:\\langchain\\langchainenv\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (1.10.13)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in a:\\langchain\\langchainenv\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in a:\\langchain\\langchainenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in a:\\langchain\\langchainenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in a:\\langchain\\langchainenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (4.0.3)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
      "  Using cached protobuf-5.27.3-cp39-cp39-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in a:\\langchain\\langchainenv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (68.2.2)\n",
      "Requirement already satisfied: anyio in a:\\langchain\\langchainenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (4.3.0)\n",
      "Requirement already satisfied: certifi in a:\\langchain\\langchainenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in a:\\langchain\\langchainenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.0.4)\n",
      "Requirement already satisfied: idna in a:\\langchain\\langchainenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.6)\n",
      "Requirement already satisfied: sniffio in a:\\langchain\\langchainenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in a:\\langchain\\langchainenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.14.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
      "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: click in a:\\langchain\\langchainenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (8.1.7)\n",
      "Requirement already satisfied: joblib in a:\\langchain\\langchainenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in a:\\langchain\\langchainenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.9.0)\n",
      "Requirement already satisfied: pywin32>=226 in a:\\langchain\\langchainenv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (227)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\langchain\\langchainenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in a:\\langchain\\langchainenv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.0.3)\n",
      "Requirement already satisfied: colorama in a:\\langchain\\langchainenv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in a:\\langchain\\langchainenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\langchain\\langchainenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in a:\\langchain\\langchainenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in a:\\langchain\\langchainenv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.2.0)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in a:\\langchain\\langchainenv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in a:\\langchain\\langchainenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-qdrant) (1.16.0)\n",
      "Downloading llama_index_vector_stores_qdrant-0.2.15-py3-none-any.whl (11 kB)\n",
      "Downloading qdrant_client-1.11.0-py3-none-any.whl (258 kB)\n",
      "   ---------------------------------------- 0.0/258.9 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 122.9/258.9 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.9/258.9 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Using cached protobuf-5.27.3-cp39-cp39-win_amd64.whl (426 kB)\n",
      "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: protobuf, hyperframe, hpack, h2, qdrant-client, llama-index-vector-stores-qdrant\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "Successfully installed h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 llama-index-vector-stores-qdrant-0.2.15 protobuf-5.27.3 qdrant-client-1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (a:\\langchain\\langchainenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (a:\\langchain\\langchainenv\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
      "google-api-core 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
      "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
      "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.3 which is incompatible.\n",
      "weaviate-client 4.5.7 requires pydantic<3.0.0,>=2.5.0, but you have pydantic 1.10.13 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-gemini\n",
      "  Using cached llama_index_llms_gemini-0.2.0-py3-none-any.whl.metadata (734 bytes)\n",
      "Collecting google-generativeai<0.6.0,>=0.5.2 (from llama-index-llms-gemini)\n",
      "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-llms-gemini) (0.10.65)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-llms-gemini) (10.3.0)\n",
      "Collecting google-ai-generativelanguage==0.6.4 (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_ai_generativelanguage-0.6.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-api-python-client (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_api_python_client-2.140.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: protobuf in a:\\gpt\\gpt\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.25.3)\n",
      "Requirement already satisfied: pydantic in a:\\gpt\\gpt\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.7.4)\n",
      "Requirement already satisfied: tqdm in a:\\gpt\\gpt\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in a:\\gpt\\gpt\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in a:\\gpt\\gpt\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2024.6.0)\n",
      "Requirement already satisfied: httpx in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (3.1)\n",
      "Requirement already satisfied: nltk>=3.8.2 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (3.8.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.24.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.40.6)\n",
      "Requirement already satisfied: pandas in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (0.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in a:\\gpt\\gpt\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in a:\\gpt\\gpt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (4.0.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in a:\\gpt\\gpt\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (5.3.3)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: click in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in a:\\gpt\\gpt\\lib\\site-packages (from nltk>=3.8.2->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2024.4.28)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (0.5.0)\n",
      "Requirement already satisfied: sniffio in a:\\gpt\\gpt\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: certifi in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.0.5)\n",
      "Requirement already satisfied: idna in a:\\gpt\\gpt\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in a:\\gpt\\gpt\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in a:\\gpt\\gpt\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in a:\\gpt\\gpt\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\gpt\\gpt\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in a:\\gpt\\gpt\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in a:\\gpt\\gpt\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (3.0.3)\n",
      "Requirement already satisfied: colorama in a:\\gpt\\gpt\\lib\\site-packages (from tqdm->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in a:\\gpt\\gpt\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in a:\\gpt\\gpt\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (3.21.3)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in a:\\gpt\\gpt\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in a:\\gpt\\gpt\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.2.1)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading grpcio-1.65.4-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in a:\\gpt\\gpt\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (24.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in a:\\gpt\\gpt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached llama_index_llms_gemini-0.2.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.4-py3-none-any.whl (679 kB)\n",
      "Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 112.6/139.4 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 139.4/139.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "   ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 143.4/200.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 200.5/200.5 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading google_api_python_client-2.140.0-py2.py3-none-any.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.1 MB 3.6 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.5/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/12.1 MB 3.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.1/12.1 MB 3.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.2/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.5/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.7/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/12.1 MB 3.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.0/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.2/12.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.3/12.1 MB 3.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.1 MB 3.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.7/12.1 MB 3.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.7/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.9/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.1/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.3/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.4/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.6/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.8/12.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.1/12.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.7/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.9/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.1/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.4/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.4/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.5/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.8/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.1/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.3/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.1/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.8/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.1/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.3/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.4/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 3.8 MB/s eta 0:00:00\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.0 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 112.6/220.0 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 220.0/220.0 kB 2.7 MB/s eta 0:00:00\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 1.2 MB/s eta 0:00:00\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.65.4-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/4.1 MB 3.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.3/4.1 MB 3.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.4/4.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.6/4.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.8/4.1 MB 3.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.9/4.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.1/4.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.2/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.4/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.5/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.6/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.8/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.0/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.1/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.3/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 2.5/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.6/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.8/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.0/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.1/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.2/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.4/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.6/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.8/4.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.8/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.0/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.1/4.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, proto-plus, grpcio, googleapis-common-protos, rsa, pyasn1-modules, httplib2, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, llama-index-llms-gemini\n",
      "Successfully installed google-ai-generativelanguage-0.6.4 google-api-core-2.19.1 google-api-python-client-2.140.0 google-auth-2.33.0 google-auth-httplib2-0.2.0 google-generativeai-0.5.4 googleapis-common-protos-1.63.2 grpcio-1.65.4 grpcio-status-1.62.3 httplib2-0.22.0 llama-index-llms-gemini-0.2.0 proto-plus-1.24.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pyparsing-3.1.2 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llama-index google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    "    ServiceContext,\n",
    "    Document\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ## SentenceWindowNodeParser:\n",
    "\n",
    "Breaks down text into overlapping windows of sentences. Useful for capturing context within a larger text. Example: If the window size is 3, it would create nodes like: Sentence 1, Sentence 2, Sentence 3 Sentence 2, Sentence 3, Sentence 4 Sentence 3, Sentence 4, Sentence 5 \n",
    "\n",
    "2. ## HierarchicalNodeParser:\n",
    "\n",
    "Creates a hierarchical structure of nodes from text. Useful for organizing information into categories or subcategories. Example: A document with sections and subsections would be represented as a hierarchical structure. \n",
    "\n",
    "3. ## get_leaf_nodes:\n",
    "\n",
    "A function that likely returns the leaf nodes from a hierarchical structure. Leaf nodes are the terminal nodes without children.\n",
    "\n",
    "Potential Use Cases: SentenceWindowNodeParser: For creating embeddings of text chunks with overlapping context. For building a search engine that can answer questions based on sentence-level context.\n",
    "\n",
    "HierarchicalNodeParser: For organizing large documents or datasets into a structured format. For creating a knowledge graph or ontology.\n",
    "\n",
    "get_leaf_nodes: For extracting the most granular information from a hierarchical structure. For performing specific operations on the lowest level nodes.\n",
    "\n",
    "Would you like to delve deeper into a specific class or use case? I can provide more detailed explanations or examples based on your requirements.\n",
    "\n",
    "Additionally, if you have a specific dataset or task in mind, I can help you determine the most suitable node parser and how to use it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Understanding SentenceSplitter\n",
    "\n",
    "SentenceSplitter is a class from the Llama Index library that is used to break down text into smaller, more manageable chunks based on sentence boundaries. This is a crucial step in many natural language processing tasks, including document indexing, summarization, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "     \n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is MetadataMode?\n",
    "\n",
    "- MetadataMode is an enum defined in the Llama Index library to control how metadata is handled when interacting with documents or nodes. Essentially, it determines which metadata fields are included or excluded when processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.schema import MetadataMode\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Class\n",
    "\n",
    "As we've established, the MetadataReplacementPostProcessor is used to replace node content with a specific metadata field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ayush\\AppData\\Local\\llama_index\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyBS0SyVVwGRFlccPR-hds7AucIYbJV6ZCw'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_MODEL_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemini(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001520751A370>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x00000152751BC0D0>, completion_to_prompt=<function default_completion_to_prompt at 0x0000015275257310>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='models/gemini-pro', temperature=0.1, max_tokens=2048, generate_kwargs={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "llm = Gemini(model=\"models/gemini-pro\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Hello there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
