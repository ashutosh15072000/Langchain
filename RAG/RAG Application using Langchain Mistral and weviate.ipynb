{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Weaviate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import weaviate\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "WEAVIATE_API_KEY=os.getenv('weaviateapikey')\n",
    "WEAVIATE_CLUSTER=os.getenv('WEAVIATECLUSTER')\n",
    "huggingfacehub_api_token=os.getenv('huggingfacehubapitoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://mylangchainproject-ilhnm5qv.weaviate.network'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEAVIATE_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\weaviate\\warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    url = WEAVIATE_CLUSTER,\n",
    "    auth_client_secret=weaviate.auth.AuthApiKey(api_key=WEAVIATE_API_KEY),  # Replace with your Weaviate instance API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify embedding model using huggingface\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n",
      "Windows platform detected, try to use DirectML as primary provider\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"Data_source/Retrieval-Augmented Generation for Knowledge intensive NLP tasks.pdf\",extract_images=True)\n",
    "pages=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=20)\n",
    "docs=text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db=Weaviate.from_documents(\n",
    "    docs,embeddings,client=client,by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\n",
      "mechanisms may not be necessary for RAG.\n",
      "G Parameters\n",
      "Our RAG models contain the trainable parameters for the BERT-base query and document encoder of\n",
      "DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n",
      "406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(vector_db.similarity_search(\"What is rag?\",k=3)[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are an assistant for question-ansering tasks.\n",
    "Use the following pieces of retrieved context to answer the quetsion.\n",
    "If you don't know the answer , just say that you don't know\n",
    "USe twenty sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Anwser:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=HuggingFaceHub(\n",
    "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser=StrOutputParser()\n",
    "retriever=vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=({\n",
    "    \"context\":retriever,\"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-ansering tasks.\n",
      "Use the following pieces of retrieved context to answer the quetsion.\n",
      "If you don't know the answer , just say that you don't know\n",
      "USe twenty sentences maximum and keep the answer concise.\n",
      "Question: What is Rag System?\n",
      "Context: [Document(page_content='Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6', metadata={'page': 5, 'source': 'Data_source/Retrieval-Augmented Generation for Knowledge intensive NLP tasks.pdf'}), Document(page_content='documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18', metadata={'page': 17, 'source': 'Data_source/Retrieval-Augmented Generation for Knowledge intensive NLP tasks.pdf'}), Document(page_content='cases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,', metadata={'page': 5, 'source': 'Data_source/Retrieval-Augmented Generation for Knowledge intensive NLP tasks.pdf'}), Document(page_content='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in', metadata={'page': 9, 'source': 'Data_source/Retrieval-Augmented Generation for Knowledge intensive NLP tasks.pdf'})]\n",
      "Anwser:\n",
      "The RAG system, as mentioned in the documents, is a Retrieval-Augmented Generation model. It is a machine learning model that combines retrieval and generation components. The system uses BERT-base query and document encoder of DPR and BART-large for its models, with a total of 626M trainable parameters. RAG does not require intermediate retrieval supervision like state-of-the-art models do. It outperforms\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"What is Rag System?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Rag System?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Rag System?\")['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
