{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: Character Splitting\n",
    "\n",
    "##### Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character size chunks regardless of their content form.\n",
    "##### This method isn't recommended for any application - but it's a great starting point for us to understand the basics \n",
    "\n",
    "- **Pros**: Easy & Simple.\n",
    "- **Cons**: Very rigid and doesn't take into account the structure of your text. \n",
    "\n",
    "Concepts to Know\n",
    "\n",
    "- **Chunk Size** - The number of character you would like in your chunks 50,100,10000 etc.\n",
    "- **Chunk Overlap** - The amount you would like your sequentail chunks to overlap. This is to try to avoid the cutting a single piece of context into mutiple pieces.This will create duplicate data across chunks.\n",
    "\n",
    "First lets get some sample text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets split this text manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a list that will hold your chunks\n",
    "chunks=[]\n",
    "chunk_size=35 # character\n",
    "# run through the range with the lenght of your data and iterate every chunk_size you want\n",
    "\n",
    "for i in range(0,len(text),chunk_size):\n",
    "    chunk=text[i:i+chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratualtions! You just split first text. We have long way to go but you are already making progress. Feel like a language model practitionor yet?\n",
    "When working with text in the language model world , we don't deal with raw strings. It is more common to work with documents.Documents are objects that hold the text you are concerned with, but also additional metadata which makes filtering and manipulation eassier later.\n",
    "\n",
    "We could convert our list of stings into documents but i'd rather start from scratch and create the docs\n",
    "\n",
    "Let's load up Langchain ` CharacterSplitter` to do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets load up this text splitter. I need to specify `Chunk_overlap` and `separator` or else we will get funk results.we will get inot next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=CharacterTextSplitter(separator='',chunk_size=chunk_size,strip_whitespace=False,chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we can actually split our text via `create_documents`. Note: `create_documents` expects a list of texts, so if you have a string (like we do)\n",
    "you will need to wrap it in `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to ch'),\n",
       " Document(page_content='unk up. It is the example text for '),\n",
       " Document(page_content='this exercise')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this time we have the same chunks, but they are in documents. These will play nicely with rest of the Langchain world.Also notice how the trailling whitespace on the end of the 2nd chunk is missing. This is because Langchain remove it ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunk Overlap & Separator**\n",
    "\n",
    "**Chunk Overlap** will blend together our chunks so that the tail of chunk #1 will be same thing and the head of chunk #2 and so on ans so forth.\n",
    "\n",
    "This time I will load up my overlap with value of 4 this means 4 charcters of overlap\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=CharacterTextSplitter(chunk_size=35,chunk_overlap=4,separator='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to ch'),\n",
       " Document(page_content='o chunk up. It is the example text'),\n",
       " Document(page_content='ext for this exercise')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3. The 'o ch' on the tail of Chunk #1 matches the 'o ch' of the head of Chunk #2.\n",
    "\n",
    "\n",
    "I wanted a better way to visualize this, so I made [ChunkViz.com](www.chunkviz.com) to help show it. Here's what the same text looks like.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static\\ChunkVizCharacter34_4_w_overlap.png\" alt=\"image\" style=\"max-width: 400px;\">\n",
    "</div>\n",
    "\n",
    "static/ChunkVizCharacterRecursive.png\n",
    "\n",
    "Check out how we have three colors, with two overlaping sections.\n",
    "\n",
    "**Separators** are character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: Recursive Character Text Splitting\n",
    "\n",
    "Let's  jump a level of complexity\n",
    "\n",
    "The problem with Level#1 is that we don't take account the structure of our document at all. We simply split by a fix number of character.\n",
    "\n",
    "The Recursive Character text splitter helps with this. With it , will specify a series of separators which will be used to split our docs.\n",
    "\n",
    "You can see the default separator for langchain here.Let's take a look at them one by one.\n",
    "\n",
    "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "* \"\\n\" - New lines\n",
    "* \" \" - Spaces\n",
    "* \"\" - Characters\n",
    "\n",
    "\n",
    "I'm not sure why a period (\".\") isn't included on the list, perhaps it is not universal enough? If you know, let me know.\n",
    "\n",
    "This is the swiss army knife of splitters and my first choice when mocking up a quick application. If you don't know which splitter to start with, this is a good first bet.\n",
    "\n",
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets load up a larger piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=65,\n",
    "    chunk_overlap=0  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the\"),\n",
       " Document(page_content='world when I was a child is the degree to which the returns for'),\n",
       " Document(page_content='performance are superlinear.'),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear.'),\n",
       " Document(page_content='\"You get out,\" I heard a thousand times, \"what you put in.\" They'),\n",
       " Document(page_content='meant well, but this is rarely true. If your product is only'),\n",
       " Document(page_content=\"half as good as your competitor's, you don't get half as many\"),\n",
       " Document(page_content='customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are\"),\n",
       " Document(page_content='superlinear in business. Some think this is a flaw of'),\n",
       " Document(page_content='capitalism, and that if we changed the rules it would stop being'),\n",
       " Document(page_content='true. But superlinear returns for performance are a feature of'),\n",
       " Document(page_content=\"the world, not an artifact of rules we've invented. We see the\"),\n",
       " Document(page_content='same pattern in fame, power, military victories, knowledge, and'),\n",
       " Document(page_content='even benefit to humanity. In all of these, the rich get richer.'),\n",
       " Document(page_content='[1]')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
    "\n",
    "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
    "\n",
    "For text of this size, let's split on something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this text, 450 splits the paragraphs perfectly. You can even switch the chunk size to 469 and get the same splits. This is because this splitter builds in a bit of cushion and wiggle room to allow your chunks to 'snap' to the nearest separator.\n",
    "\n",
    "Let's view this visually\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static\\ChunkVizCharacterRecursive.png\" alt=\"image\" style=\"max-width: 300px;\">\n",
    "</div>\n",
    "\n",
    "Wow - you already made it to level 2, awesome! We're on a roll. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 3: Document Specific Splitting \n",
    "\n",
    "Stepping up our level ladder,lets start to handle document types other than normal prose in a .text what if you have picture? or a PDF? or Code Snippets?\n",
    "\n",
    "Our first two level wouldn't work great for this so will need to find a different tactic.\n",
    "\n",
    "\n",
    "This \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: Document Specific Splitting\n",
    "\n",
    "Stepping up our levels ladder let's start to handle document types other than normal in a .txt what if you have picture? or a PDf? or code snippets?\n",
    "\n",
    "our first levels wouldn't work great for this so we will need to find a different tatic.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats.Let's run through a bunch of examples of this in action.\n",
    "\n",
    "The Markdown,Python and Js splitter will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "See all of Langchain Document spliiter Here ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown\n",
    "\n",
    "**Separator**\n",
    " You can see the separator\n",
    "\n",
    " - \\n#{1,6} - Split by new lines followed by a header(H1 through H6)\n",
    " - ```\\n - conde blocks\n",
    " - \\n\\\\*\\\\*\\\\8+\\n - Horizontal Lines\n",
    " - \\n---+\\n - Horizontal Lines\n",
    " - \\n__+\\n -Horizontal Lines\n",
    " - \\n\\n - Double Lines\n",
    " - \" \" - Spaces\n",
    " - \"\" - Character\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter=MarkdownTextSplitter(chunk_size=40,chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Fun in California\\n\\n## Driving'),\n",
       " Document(page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(page_content='### Food'),\n",
       " Document(page_content=\"Make sure to eat a burrito while you're\"),\n",
       " Document(page_content='there'),\n",
       " Document(page_content='## Hiking\\n\\nGo to Yosemite')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spliter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the splits gravitate towards markdown sections. However, it's still not perfect. Check out how there is a chunk with just \"there\" in it. You'll run into this at low-sized chunks.\n",
    "\n",
    "### Python\n",
    "\n",
    "See the python splitters [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1069)\n",
    "\n",
    "* `\\nclass` - Classes first\n",
    "* `\\ndef` - Functions next\n",
    "* `\\n\\tdef` - Indented functions\n",
    "* `\\n\\n` - Double New lines\n",
    "* `\\n` - New Lines\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Characters\n",
    "\n",
    "\n",
    "Let's load up our splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text= \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = PythonCodeTextSplitter(chunk_size=50, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='class Person:\\n  def __init__(self, name, age):'),\n",
       " Document(page_content='self.name = name\\n    self.age = age'),\n",
       " Document(page_content='p1 = Person(\"John\", 36)'),\n",
       " Document(page_content='for i in range(10):\\n    print (i)')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how the class stays together in a single document (good), then the rest of the code is in a second document (ok).\n",
    "\n",
    "I needed to play with the chunk size to get a clean result like that. You'll likely need to do the same for yours which is why using evaluations to determine optimal chunk sizes is crucial.\n",
    "\n",
    "### JS\n",
    "\n",
    "Very similar to python. See the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L983).\n",
    "\n",
    "Separators:\n",
    "* `\\nfunction` - Indicates the beginning of a function declaration\n",
    "* `\\nconst` - Used for declaring constant variables\n",
    "* `\\nlet` - Used for declaring block-scoped variables\n",
    "* `\\nvar` - Used for declaring a variable\n",
    "* `\\nclass` - Indicates the start of a class definition\n",
    "* `\\nif` - Indicates the beginning of an if statement\n",
    "* `\\nfor` - Used for for-loops\n",
    "* `\\nwhile` - Used for while-loops\n",
    "* `\\nswitch` - Used for switch statements\n",
    "* `\\ncase` - Used within switch statements\n",
    "* `\\ndefault` - Also used within switch statements\n",
    "* `\\n\\n` - Indicates a larger separation in text or code\n",
    "* `\\n` - Separates lines of code or text\n",
    "* `\" \"` - Separates words or tokens in the code\n",
    "* `\"\"` - Makes every character a separate element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_text = \"\"\"\n",
    "// Function is called, the return value will end up in x\n",
    "let x = myFunction(4, 3);\n",
    "\n",
    "function myFunction(a, b) {\n",
    "// Function returns the product of a and b\n",
    "  return a * b;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='// Function is called, the return value will end up in x'),\n",
       " Document(page_content='let x = myFunction(4, 3);'),\n",
       " Document(page_content='function myFunction(a, b) {'),\n",
       " Document(page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_splitter.create_documents([javascript_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFs w/ tables\n",
    "\n",
    "Ok now things will get a bit spicier.\n",
    "\n",
    "PDFs are an extremely common data type for language model work. Often they'll contain tables that contain information.\n",
    "\n",
    "This could be financial data, studies, academic papers, etc.\n",
    "\n",
    "Trying to split tables by a character based separator isn't reliable. We need to try out a different method. For a deep dive on this I recommend checking out [Lance Martin's](https://twitter.com/RLanceMartin) [tutorial](https://twitter.com/RLanceMartin/status/1721942636364456336) w/ LangChain.\n",
    "\n",
    "I'll be going through a text based methods. [Mayo](https://twitter.com/mayowaoshin) has also outlined a GPT-4V method which tries to pulls tables via vision rather than text. You can check out [here](https://twitter.com/mayowaoshin/status/1727399231734886633).\n",
    "\n",
    "A very convenient way to do this is with [Unstructured](https://unstructured.io/), a library dedicated to making your data LLM ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up our PDF and then parition it. This is a PDF from a [Salesforce earning report](https://investor.salesforce.com/financials/default.aspx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\pdf2image\\pdf2image.py:581\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    580\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poppler_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 581\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic/SalesforceFinancial.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## extract the elements from the Pdf\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m elements\u001b[38;5;241m=\u001b[39m\u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m##unstructure Helpers\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi_res\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\documents\\elements.py:539\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 539\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    540\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[0;32m    541\u001b[0m     params: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\file_utils\\filetype.py:622\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 622\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[0;32m    624\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\file_utils\\filetype.py:582\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 582\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    583\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[0;32m    584\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\chunking\\dispatch.py:83\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_args\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[0;32m     86\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults()\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\partition\\pdf.py:217\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[1;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, include_metadata, metadata_filename, metadata_last_modified, chunking_strategy, links, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, date_from_file_object, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m exactly_one(filename\u001b[38;5;241m=\u001b[39mfilename, file\u001b[38;5;241m=\u001b[39mfile)\n\u001b[0;32m    215\u001b[0m languages \u001b[38;5;241m=\u001b[39m check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages) \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m partition_pdf_or_image(\n\u001b[0;32m    218\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    219\u001b[0m     file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m    220\u001b[0m     include_page_breaks\u001b[38;5;241m=\u001b[39minclude_page_breaks,\n\u001b[0;32m    221\u001b[0m     strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    222\u001b[0m     infer_table_structure\u001b[38;5;241m=\u001b[39minfer_table_structure,\n\u001b[0;32m    223\u001b[0m     languages\u001b[38;5;241m=\u001b[39mlanguages,\n\u001b[0;32m    224\u001b[0m     metadata_last_modified\u001b[38;5;241m=\u001b[39mmetadata_last_modified,\n\u001b[0;32m    225\u001b[0m     hi_res_model_name\u001b[38;5;241m=\u001b[39mhi_res_model_name,\n\u001b[0;32m    226\u001b[0m     extract_images_in_pdf\u001b[38;5;241m=\u001b[39mextract_images_in_pdf,\n\u001b[0;32m    227\u001b[0m     extract_image_block_types\u001b[38;5;241m=\u001b[39mextract_image_block_types,\n\u001b[0;32m    228\u001b[0m     extract_image_block_output_dir\u001b[38;5;241m=\u001b[39mextract_image_block_output_dir,\n\u001b[0;32m    229\u001b[0m     extract_image_block_to_payload\u001b[38;5;241m=\u001b[39mextract_image_block_to_payload,\n\u001b[0;32m    230\u001b[0m     date_from_file_object\u001b[38;5;241m=\u001b[39mdate_from_file_object,\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    232\u001b[0m )\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\partition\\pdf.py:305\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[1;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, date_from_file_object, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    304\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 305\u001b[0m         elements \u001b[38;5;241m=\u001b[39m _partition_pdf_or_image_local(\n\u001b[0;32m    306\u001b[0m             filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    307\u001b[0m             file\u001b[38;5;241m=\u001b[39mspooled_to_bytes_io_if_needed(file),\n\u001b[0;32m    308\u001b[0m             is_image\u001b[38;5;241m=\u001b[39mis_image,\n\u001b[0;32m    309\u001b[0m             infer_table_structure\u001b[38;5;241m=\u001b[39minfer_table_structure,\n\u001b[0;32m    310\u001b[0m             include_page_breaks\u001b[38;5;241m=\u001b[39minclude_page_breaks,\n\u001b[0;32m    311\u001b[0m             languages\u001b[38;5;241m=\u001b[39mlanguages,\n\u001b[0;32m    312\u001b[0m             metadata_last_modified\u001b[38;5;241m=\u001b[39mmetadata_last_modified \u001b[38;5;129;01mor\u001b[39;00m last_modification_date,\n\u001b[0;32m    313\u001b[0m             hi_res_model_name\u001b[38;5;241m=\u001b[39mhi_res_model_name,\n\u001b[0;32m    314\u001b[0m             pdf_text_extractable\u001b[38;5;241m=\u001b[39mpdf_text_extractable,\n\u001b[0;32m    315\u001b[0m             extract_images_in_pdf\u001b[38;5;241m=\u001b[39mextract_images_in_pdf,\n\u001b[0;32m    316\u001b[0m             extract_image_block_types\u001b[38;5;241m=\u001b[39mextract_image_block_types,\n\u001b[0;32m    317\u001b[0m             extract_image_block_output_dir\u001b[38;5;241m=\u001b[39mextract_image_block_output_dir,\n\u001b[0;32m    318\u001b[0m             extract_image_block_to_payload\u001b[38;5;241m=\u001b[39mextract_image_block_to_payload,\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    320\u001b[0m         )\n\u001b[0;32m    321\u001b[0m         out_elements \u001b[38;5;241m=\u001b[39m _process_uncategorized_text_elements(elements)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m PartitionStrategy\u001b[38;5;241m.\u001b[39mFAST:\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\utils.py:220\u001b[0m, in \u001b[0;36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_deps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollowing dependencies are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_deps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m         ),\n\u001b[0;32m    219\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured\\partition\\pdf.py:430\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_local\u001b[1;34m(filename, file, is_image, infer_table_structure, include_page_breaks, languages, ocr_mode, model_name, hi_res_model_name, pdf_image_dpi, metadata_last_modified, pdf_text_extractable, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, analysis, analyzed_image_output_dir_path, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Chipper model performs better when images are rendered with DPI >= 300 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(currently \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_image_dpi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    427\u001b[0m     )\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m     inferred_document_layout \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file_with_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hi_res_model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchipper\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;66;03m# NOTE(alan): We shouldn't do OCR with chipper\u001b[39;00m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;66;03m# NOTE(antonio): We shouldn't do PDFMiner with chipper\u001b[39;00m\n\u001b[0;32m    440\u001b[0m         final_document_layout \u001b[38;5;241m=\u001b[39m inferred_document_layout\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured_inference\\inference\\layout.py:394\u001b[0m, in \u001b[0;36mprocess_file_with_model\u001b[1;34m(filename, model_name, is_image, fixed_layouts, pdf_image_dpi, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    386\u001b[0m layout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     DocumentLayout\u001b[38;5;241m.\u001b[39mfrom_image_file(\n\u001b[0;32m    388\u001b[0m         filename,\n\u001b[0;32m    389\u001b[0m         detection_model\u001b[38;5;241m=\u001b[39mdetection_model,\n\u001b[0;32m    390\u001b[0m         element_extraction_model\u001b[38;5;241m=\u001b[39melement_extraction_model,\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_image\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m DocumentLayout\u001b[38;5;241m.\u001b[39mfrom_file(\n\u001b[0;32m    395\u001b[0m         filename,\n\u001b[0;32m    396\u001b[0m         detection_model\u001b[38;5;241m=\u001b[39mdetection_model,\n\u001b[0;32m    397\u001b[0m         element_extraction_model\u001b[38;5;241m=\u001b[39melement_extraction_model,\n\u001b[0;32m    398\u001b[0m         fixed_layouts\u001b[38;5;241m=\u001b[39mfixed_layouts,\n\u001b[0;32m    399\u001b[0m         pdf_image_dpi\u001b[38;5;241m=\u001b[39mpdf_image_dpi,\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m )\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layout\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured_inference\\inference\\layout.py:64\u001b[0m, in \u001b[0;36mDocumentLayout.from_file\u001b[1;34m(cls, filename, fixed_layouts, pdf_image_dpi, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading PDF for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m temp_dir:\n\u001b[1;32m---> 64\u001b[0m     _image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_pdf_to_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf_image_dpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     image_paths \u001b[38;5;241m=\u001b[39m cast(List[\u001b[38;5;28mstr\u001b[39m], _image_paths)\n\u001b[0;32m     71\u001b[0m     number_of_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_paths)\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\unstructured_inference\\inference\\layout.py:431\u001b[0m, in \u001b[0;36mconvert_pdf_to_image\u001b[1;34m(filename, dpi, output_folder, path_only)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_folder must be specified if path_only is true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mpdf2image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m     images \u001b[38;5;241m=\u001b[39m pdf2image\u001b[38;5;241m.\u001b[39mconvert_from_path(\n\u001b[0;32m    439\u001b[0m         filename,\n\u001b[0;32m    440\u001b[0m         dpi\u001b[38;5;241m=\u001b[39mdpi,\n\u001b[0;32m    441\u001b[0m         paths_only\u001b[38;5;241m=\u001b[39mpath_only,\n\u001b[0;32m    442\u001b[0m     )\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\pdf2image\\pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[1;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n\u001b[0;32m    125\u001b[0m     poppler_path \u001b[38;5;241m=\u001b[39m poppler_path\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m--> 127\u001b[0m page_count \u001b[38;5;241m=\u001b[39m \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoppler_path\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n\u001b[0;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[38;5;241m=\u001b[39m _parse_format(\n\u001b[0;32m    133\u001b[0m     fmt, grayscale\n\u001b[0;32m    134\u001b[0m )\n",
      "File \u001b[1;32ma:\\LANGCHAIN\\langchainenv\\lib\\site-packages\\pdf2image\\pdf2image.py:607\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     )\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "filename = Path(\"static/SalesforceFinancial.pdf\")\n",
    "\n",
    "## extract the elements from the Pdf\n",
    "\n",
    "elements=partition_pdf(\n",
    "    filename=filename,\n",
    "    ##unstructure Helpers\n",
    "    strategy='hi_res',\n",
    "    infer_table_structure=True,\n",
    "    model_name='yolox'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's look at our elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just unstructured objects, we could look at them all but I want to look at the table it parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[-4].metadata.text_as_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
